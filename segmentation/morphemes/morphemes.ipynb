{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpheme Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Turkish file: newstest2017.tr\n",
    "- Tokenize sentences using the Zemberek tool (you can use tokenize_zemberek.java)  --> newstest2017.zb.tr\n",
    "- Clean and truecase using the Moses scripts --> newstest2017.zb.tc.tr\n",
    "- Apply morphological parsing -->  newstest2017.zb.mp.tr   (https://github.com/BOUN-TABILab-TULAP/Morphological-Parser)\n",
    "- Apply morphological disambiguation   --> newstest2017.zb.md.tr  (https://github.com/BOUN-TABILab-TULAP/Morphological-Parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morpheme segmentation (Morph)\n",
    "\n",
    "Use the **segment_morphemes** function for segmenting a Turkish word into its morphemes:\n",
    "\n",
    "evdekiler --> ev DA ki LAR\n",
    "\n",
    "To use this function, you should provide the word-tokenized file (newstest2017.zb.tr) and morphologically disambiguated file (newstest2017.zb.md.tr). The generated file will be newstest2017.zb.mdnn.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_morphemes(filename):\n",
    "    with open(filename, encoding='utf-8') as corpus_file:\n",
    "        lines = corpus_file.readlines()\n",
    "    with open(filename.replace(\".md\", \"\"), encoding='utf-8') as corpus_file:\n",
    "        original_lines = corpus_file.readlines()\n",
    "    sentence_count = 0\n",
    "    with open(filename.replace(\".md\", \".mdnn\"), \"w\", encoding='utf-8') as corpus_file:\n",
    "        for line in lines:\n",
    "            if \"BSTag\" in line:\n",
    "                sentence = \"\"\n",
    "                original_sentence = original_lines[sentence_count]\n",
    "                original_tokens = original_sentence.strip().split()\n",
    "                token_count = 0\n",
    "            elif \"ESTag\" in line:\n",
    "                corpus_file.write(sentence.strip() + \"\\n\")\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                casedRoot = line.split()[0]\n",
    "                morph_analysis = line.split()[1]\n",
    "                morph_list = [m.split(\"[\") for m in morph_analysis.split(\"]\")]\n",
    "                for i, token in enumerate(morph_list):\n",
    "                    try:\n",
    "                        word, tag = token\n",
    "                        word = word.strip(\"-\").strip(\"+\")\n",
    "                        #print(word, end=\" \")\n",
    "                        if word != '':\n",
    "                            if word != \"\\'\" and \"\\'\" in word:\n",
    "                                sequence = normalize_apos_suffices(word)\n",
    "                                if sequence == []:\n",
    "                                    first, second = word.split(\"\\'\")\n",
    "                                    sequence = [first, \"\\'\", second]\n",
    "                                for s in sequence:\n",
    "                                    sentence += s + \" \"\n",
    "                            elif word != \"’\" and \"’\" in word:\n",
    "                                sequence = normalize_apos_suffices(word)\n",
    "                                if sequence == []:\n",
    "                                    first, second = word.split(\"’\")\n",
    "                                    sequence = [first, \"’\", second]\n",
    "                                for s in sequence:\n",
    "                                    sentence += s + \" \"\n",
    "                            elif i == 0:\n",
    "                                original_current_token = original_tokens[token_count]\n",
    "                                if original_current_token[0].isupper():\n",
    "                                    sentence += original_current_token[0] + word[1:] + \" \"\n",
    "                                else:\n",
    "                                    sentence += word + \" \"\n",
    "                            else:\n",
    "                                sentence += word + \" \"\n",
    "                    except:\n",
    "                        \"\"\"print(line)\n",
    "                        print(token[0].split(\"[\"))\"\"\"\n",
    "                token_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_morphemes(\"newstest2017.zb.md.tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated morpheme segmentation (ConcatMorph)\n",
    "\n",
    "Use the **segment_morphemes_all** function for segmenting a Turkish word into its morphemes, where all the morphemes except for the root are concatenated:\n",
    "\n",
    "evdekiler --> ev DAkiLAR\n",
    "\n",
    "To use this function, you should provide the word-tokenized file (newstest2017.zb.tr) and morphologically disambiguated file (newstest2017.zb.md.tr). The generated file will be newstest2017.zb.mdnnall.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_morphemes_all(filename):\n",
    "    with open(filename, encoding='utf-8') as corpus_file:\n",
    "        lines = corpus_file.readlines()\n",
    "    with open(filename.replace(\".md\", \"\"), encoding='utf-8') as corpus_file:\n",
    "        original_lines = corpus_file.readlines()\n",
    "    sentence_count = 0\n",
    "    with open(filename.replace(\".md\", \".mdnnall\"), \"a+\", encoding='utf-8') as corpus_file:\n",
    "        for line in lines:\n",
    "            if \"BSTag\" in line:\n",
    "                sentence = \"\"\n",
    "                original_sentence = original_lines[sentence_count]\n",
    "                original_tokens = original_sentence.strip().split()\n",
    "                token_count = 0\n",
    "            elif \"ESTag\" in line:\n",
    "                corpus_file.write(sentence.strip() + \"\\n\")\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                casedRoot = line.split()[0]\n",
    "                morph_analysis = line.split()[1]\n",
    "                morph_list = [m.split(\"[\") for m in morph_analysis.split(\"]\")]\n",
    "                morph_token_count = 0\n",
    "                for i, token in enumerate(morph_list):\n",
    "                    try:\n",
    "                        word, tag = token\n",
    "                        word = word.strip(\"-\").strip(\"+\")\n",
    "                        #print(word, end=\" \")\n",
    "                        if word != '':\n",
    "                            if word != \"\\'\" and \"\\'\" in word:\n",
    "                                sequence = normalize_apos_suffices(word)\n",
    "                                if sequence == []:\n",
    "                                    first, second = word.split(\"\\'\")\n",
    "                                    sequence = [first, \"\\'\", second]\n",
    "                                for s in sequence:\n",
    "                                    sentence += s + \" \"\n",
    "                            elif i == 0:\n",
    "                                original_current_token = original_tokens[token_count]\n",
    "                                if original_current_token[0].isupper():\n",
    "                                    sentence += original_current_token[0] + word[1:] + \" \"\n",
    "                                else:\n",
    "                                    sentence += word + \" \"\n",
    "                            elif morph_token_count == 1:\n",
    "                                if word != \"\\'\":\n",
    "                                    sentence += \"_\" + word\n",
    "                                else:\n",
    "                                    sentence += word + \" _\"\n",
    "                            else:\n",
    "                                sentence += word\n",
    "                            morph_token_count += 1\n",
    "                    except:\n",
    "                        \"\"\"print(line)\n",
    "                        print(token[0].split(\"[\"))\"\"\"\n",
    "                if sentence != \"\":\n",
    "                    if sentence[-1] != \" \":\n",
    "                        sentence += \" \"\n",
    "                token_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_morphemes_all(\"newstest2017.zb.md.tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last morpheme segmentation (LastMorph)\n",
    "\n",
    "Use the **segment_morphemes_last** function for segmenting a Turkish word into its morphemes, take the root, and only the last morpheme.\n",
    "\n",
    "evdekiler --> ev LAR\n",
    "\n",
    "To use this function, you should provide the word-tokenized file (newstest2017.zb.tr) and morphologically disambiguated file (newstest2017.zb.md.tr). The generated file will be newstest2017.zb.mdnnlast.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_morphemes_last(filename):\n",
    "    with open(filename, encoding='utf-8') as corpus_file:\n",
    "        lines = corpus_file.readlines()\n",
    "    with open(filename.replace(\".md\", \"\"), encoding='utf-8') as corpus_file:\n",
    "        original_lines = corpus_file.readlines()\n",
    "    sentence_count = 0\n",
    "    with open(filename.replace(\".md\", \".mdnnlast\"), \"a+\", encoding='utf-8') as corpus_file:\n",
    "        for line in lines:\n",
    "            if \"BSTag\" in line:\n",
    "                sentence = \"\"\n",
    "                original_sentence = original_lines[sentence_count]\n",
    "                original_tokens = original_sentence.strip().split()\n",
    "                token_count = 0\n",
    "            elif \"ESTag\" in line:\n",
    "                corpus_file.write(sentence.strip() + \"\\n\")\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                casedRoot = line.split()[0]\n",
    "                morph_analysis = line.split()[1]\n",
    "                morph_list = [m.split(\"[\") for m in morph_analysis.split(\"]\")]\n",
    "                morph_token_count = 0\n",
    "                last_suffix = \"\"\n",
    "                for i, token in enumerate(morph_list):\n",
    "                    try:\n",
    "                        word, tag = token\n",
    "                        word = word.strip(\"-\").strip(\"+\")\n",
    "                        #print(word, end=\" \")\n",
    "                        if word != '':\n",
    "                            if word != \"\\'\" and \"\\'\" in word:\n",
    "                                sequence = normalize_apos_suffices(word)\n",
    "                                if sequence == []:\n",
    "                                    first, second = word.split(\"\\'\")\n",
    "                                    sequence = [first, \"\\'\", second]\n",
    "                                for s in sequence:\n",
    "                                    sentence += s + \" \"\n",
    "                            elif i == 0:\n",
    "                                original_current_token = original_tokens[token_count]\n",
    "                                if original_current_token[0].isupper():\n",
    "                                    sentence += original_current_token[0] + word[1:] + \" \"\n",
    "                                else:\n",
    "                                    sentence += word + \" \"\n",
    "                            elif word == \"\\'\":\n",
    "                                sentence += word + \" \"\n",
    "                            else:\n",
    "                                last_suffix = word\n",
    "                            morph_token_count += 1\n",
    "                    except:\n",
    "                        \"\"\"print(line)\n",
    "                        print(token[0].split(\"[\"))\"\"\"\n",
    "                if last_suffix != \"\":\n",
    "                    sentence += \"_\" + last_suffix + \" \"\n",
    "                if sentence != \"\":\n",
    "                    if sentence[-1] != \" \":\n",
    "                        sentence += \" \"\n",
    "                token_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_morphemes_last(\"newstest2017.zb.md.tr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
